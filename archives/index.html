<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>Yilin Bao's Site</title><meta name="author" content="Yilin Bao"><link rel="shortcut icon" href="/img/channels4_profile.jpeg"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.13.0/css/all.min.css"><meta name="generator" content="Hexo 7.0.0"></head><body><header id="page_header"><div class="header_wrap"><div id="blog_name"><a class="blog_title" id="site-name" href="/">Yilin Bao's Site</a></div><button class="menus_icon"><div class="navicon"></div></button><ul class="menus_items"><li class="menus_item"><a class="site-page" href="/" target="_blank"> About</a></li><li class="menus_item"><a class="site-page" href="/#Publications" target="_blank"> Publications</a></li><li class="menus_item"><a class="site-page" href="https://yilin-bao.github.io/blog/" target="_blank"> Blog and Thinking</a></li><li class="menus_item"><a class="site-page" href="/#Proposals" target="_blank"> Research Proposals</a></li></ul></div></header><main id="page_main"><div class="side-card sticky"><div class="card-wrap" itemscope itemtype="http://schema.org/Person"><div class="author-avatar"><img class="avatar-img" src="/img/channels4_profile.jpeg" onerror="this.onerror=null;this.src='/img/channels4_profile.jpeg'" alt="avatar"></div><div class="author-discrip"><h3>Yilin Bao</h3><p class="author-bio">I am Yilin Bao, current pursing ECE master degree in UCSD, focusing on machine learning arm.</p></div><div class="author-links"><button class="btn m-social-links">Links</button><ul class="social-icons"><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-twitter" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-facebook-square" aria-hidden="true"></i></a></li><li><a class="social-icon" href="https://github.com/yilin-bao" target="_blank"><i class="fab fa-github" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-stack-overflow" aria-hidden="true"></i></a></li><li><a class="social-icon" href="https://www.linkedin.com/in/yilin-bao-759718249/" target="_blank"><i class="fab fa-linkedin" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-weibo" aria-hidden="true"></i></a></li><li><a class="social-icon" href="kica8462852" target="_blank"><i class="fab fa-weixin" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-qq" aria-hidden="true"></i></a></li><li><a class="social-icon" href="mailto:[yibao@ucsd.edu]" target="_blank"><i class="fas fa-envelope" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fas fa-rss" aria-hidden="true"></i></a></li></ul><ul class="social-links"><li><a class="e-social-link" href="/" target="_blank"><i class="fas fa-graduation-cap" aria-hidden="true"></i><span>Google Scholar</span></a></li><li><a class="e-social-link" href="/" target="_blank"><i class="fab fa-orcid" aria-hidden="true"></i><span>ORCID</span></a></li></ul></div><a class="cv-links" href="/attaches/ybcv.pdf" target="_blank"><i class="fas fa-file-pdf" aria-hidden="true"><span>My Detail CV.</span></i></a><script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=65PZ8WRJCE-Lod4QAEKkuTvVT0kBbUFQsZ3p8PP_DjU"></script></div></div><div class="page" itemscope itemtype="http://schema.org/CreativeWork"><article><h2 id="About-me"><a href="#About-me" class="headerlink" title="About me"></a>About me</h2><p>I am Yilin Bao, current pursing <a target="_blank" rel="noopener" href="https://www.ece.ucsd.edu/">ECE master degree in UCSD</a>, focusing on <a target="_blank" rel="noopener" href="https://www.ece.ucsd.edu/sites/default/files/research-areas/DP-%20MLDS%20%28EC93%29%202022-2023%20FILLABLE_4.pdf">machine learning arm</a>.</p>
<h3 id="Research-Career-Goal"><a href="#Research-Career-Goal" class="headerlink" title="Research Career Goal"></a>Research Career Goal</h3><p>My long-term research goal is about the interpretability of neural networks. I don‚Äôt just want to use constraint networks like interpretability loss to mine interpretability, I expect the changes that can be brought about by introducing more mathematical tools.</p>
<h3 id="Research-Interests"><a href="#Research-Interests" class="headerlink" title="Research Interests"></a>Research Interests</h3><p>My current research interests are natural language processing (NLP) and computer vision (CV), and what excites me most is their combination, such as work in the multi-modal direction. At the same time, I have experience related to large language models, and you can check the CV for more information.</p>
<p>I am currently conducting research on reproducing <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.14344">Siamese Masked Autoencoders</a>. Anyone who is interested is welcome to contact me to discuss. üëè <a href="yibao@ucsd.edu">yibao@ucsd.edu</a></p>
<h3 id="After-Research-Life"><a href="#After-Research-Life" class="headerlink" title="After Research Life"></a>After Research Life</h3><p>I love reading amazing travel stories, such as <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Kino%27s_Journey">Kino‚Äôs Journey</a>. I don‚Äôt like it when an author tries to tell me something about what he or she thinks about the world through the work. In contrast, an interesting setting unfolds an interesting adventure, no matter when and where the future is, reality and The collision of stories will bring new thinking. üí•</p>
<h3 id="Career-Plans"><a href="#Career-Plans" class="headerlink" title="Career Plans"></a>Career Plans</h3><p>I am currently reaching out for PhD program in 2024 fall, feel free to reach out.</p>
<h3 id="Publications"><a href="#Publications" class="headerlink" title="Publications"></a>Publications</h3><p>[1] Bethel, Brandon J. and Dong, Changming and Zhou, Shuyi and Sun, Wenjin and <strong>Bao, Yilin</strong>. ‚Äú<a target="_blank" rel="noopener" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4153300">Assessing<br>Long Short-Term Memory Network Significant Wave Height Forecast Efficacy in the Caribbean Sea and<br>Atlantic Ocean.</a>‚Äù SSRN Electrical Journal, July 2022.</p>
<blockquote>
<p>Currently, many studies have used in situ or remotely sensed observations and numerical models in wave energy assessments, and artificial intelligence (AI) forecasting. However, little attention has been paid on the efficacy of AI forecasting in different wave regimes. Using ten years of buoy observations and numerical wave model output, the significant wave height (SWH) climates of the Caribbean (CS) and Atlantic Ocean (AO) from 2010 ‚Äì 2019 are first studied. Then, six sites throughout the CS and AO are selected for forecasting using the Long Short-Term Memory (LSTM) network. Although expected, results illustrate that regardless of location, LSTM forecasts were highly accurate, reaching correlation values of &gt;0.8, root-mean-square errors &lt;0.4 m, and mean average percentage errors of &lt;14% up to 12-hr forecast horizons. More interesting was that location-specific geographic and metocean attributes led to divergent forecast outcomes between test sites. Forecast correlations were higher near, but not directly under the Caribbean Low-Level Jet, leading to best forecast results in the western CS, followed by the central CS, and was poorest in the AO. It was conclusively determined AO and CS wave fields are sufficiently different to ensure that mismatched forecast test&#x2F;training datasets would lead to high levels of error.</p>
</blockquote>
<h2 id="Proposals"><a href="#Proposals" class="headerlink" title="Proposals"></a>Proposals</h2><h3 id="Topology-Machine-learning"><a href="#Topology-Machine-learning" class="headerlink" title="Topology &amp; Machine learning"></a>Topology &amp; Machine learning</h3><p>Neural network is always recognized as one of the most famous ‚Äúblack box‚Äù model. There are attempts to explain what happened in the learning process, what makes it better, what should be avoid. Most frequently used method is add some explainable layer (heat map, attention, etc.) or use clustering. But isn‚Äôt there anything more we can think about?</p>
<div align="center">
<img src="https://d3i71xaburhd42.cloudfront.net/de01f74a899ecc7e3228cddbc743aaf6faf5e55f/4-Figure2-1.png" width="600">
</div>

<p>‚ÄúLess is more!‚Äù</p>
<p>Knowledge is about finding abstract logic (pattern) in complex reality (high-dimensional data). Traditional science make this abstraction by find independent components. For instance, state of motion (eigenstate of motion equation) from classical mechanics, principal component analysis in data science (SVD decomposition of covariance), Legendre polynomials of spherical harmonics (functions as eigen, PDE), quantum states to Schr√∂dinger‚Äôs equation (also PDE), Laplace matrix and its eigenvector of graph theory. All looking for those special and beautiful, independent and invariant components.</p>
<p>When encounter with really high dimension, like what we are dealing with in machine learning, finding matrix representation doesn‚Äôt explain that much, because there are millions of them. So, we need some invariant new, which leads me thinking about introducing topology: characters of connection only.</p>
<div align="center">
<img src="https://d3i71xaburhd42.cloudfront.net/46269c47b5affcd6b2dfd9d933b5691bf7d9dc84/5-Figure7-1.png" width="400">
</div>

<p>The early attempts of using topology to explain neural networks was focusing on convolutional neural networks (CNN). For instance, you many find the all kernels learned from CNN in the same layer makes a wonderful mainfold. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1710.04019.pdf">Topological data analysis (TDA)</a> finds its topology invariants (in terms of Betti numbers), so we will know how many variables (coordinates&#x2F;groups) we need to describe them (only 2, corresponding to 2 kinds of rotations).</p>
<div align="center">
<img src="https://d3i71xaburhd42.cloudfront.net/058bded152c14e3c63b2d0cf7558908484eed1dc/2-Figure1-1.png" width="800">
</div>

<p>More interesting pioneer works shows that activation function is the key of changing topology of embedding manifold. And full connected layers fold them to prepare for next unbuckle.</p>
<div align="center">
<img src="https://cdn.mathpix.com/snip/images/vbMprR6CyRXhf2sNqbcMpE2gd9NNWvwiboTnSV-Tcao.original.fullsize.png" width="300">
</div>

<p>I am working on followings currently</p>
<ul>
<li>Build topology analysis on more different and difficult machine learning tasks and neural network structures, the more experiment we do, the more we might find.</li>
<li>The topology of CNN is very clear now, that means I don‚Äôt need to train model from zero start. Similar to fine-tune but not exactly the same. Those kernels, by topology, gives a already-close-to-minimum position, as a result, the training will be accelerated.</li>
<li>People who familiar with <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1710.04019.pdf">TDA</a> might know, it will add a lots of not important topology when the open set grows, I want to develop an algorithm to cut off those meaningless ones, just leave those important.</li>
<li>Topology means a lot, first of all, it leads to <a target="_blank" rel="noopener" href="https://link.springer.com/book/10.1007/978-1-4757-4034-9">group algebra</a>, also by representation theory, it connects to matrix and symplectic geometry, this all means we now have a wonderful tool that can connect ML method with traditional math-science methods.</li>
<li>Topology only gives connection, that means shape, if we can find a numerical representation to <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Tangent_bundle">tangent bundle</a> of manifolds in data space (I am thinking orthogonal hyperplanes to TDA vectors), we have a Lie group related to current task. I am not sure what this would leads to know, but it is always sure: introduce more math tools &#x3D;&#x3D; more fun!</li>
<li>The mapper algorithm of TDA can reduce the number of data point while maintain the topological shape, a interesting thought is to develop an ‚Äúadvanced‚Äù control net that not only fit human shapes, but all kinds of shapes.</li>
</ul>
<div align="center">
<img src="https://www.frontiersin.org/files/Articles/667963/frai-04-667963-HTML-r3/image_m/frai-04-667963-g010.jpg" width="400">
</div>

<ul>
<li>Naitzat, Gregory, Andrey Zhitnikov, and Lek-Heng Lim. ‚ÄúTopology of Deep Neural Networks.‚Äù arXiv, April 13, 2020. <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2004.06093">http://arxiv.org/abs/2004.06093</a>.</li>
<li>Ancona, Marco, et al. ‚ÄúTowards better understanding of gradient-based attribution methods for deep neural networks.‚Äù arXiv preprint arXiv:1711.06104 (2017).</li>
<li>Zhao, Yang, and Hao Zhang. ‚ÄúA Topological Approach to Exploring Convolutional Neural Networks.‚Äù arXiv, November 2, 2020. <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2011.00789">http://arxiv.org/abs/2011.00789.</a></li>
<li>Gabrielsson, Rickard Br√ºel, and Gunnar Carlsson. ‚ÄúExposition and Interpretation of the Topology of Neural Networks.‚Äù arXiv, October 18, 2019. <a target="_blank" rel="noopener" href="http://arxiv.org/abs/1810.03234">http://arxiv.org/abs/1810.03234.</a></li>
<li>Chern, Shing-Shen. ‚ÄúFrom Triangles to Manifolds.‚Äù The American Mathematical Monthly 86, no. 5 (May 1979): 339. <a target="_blank" rel="noopener" href="https://doi.org/10.2307/2321093">https://doi.org/10.2307/2321093</a>.</li>
<li>Gabrielsson, Rickard Br√ºel, and Gunnar Carlsson. ‚ÄúExposition and Interpretation of the Topology of Neural Networks.‚Äù arXiv, October 18, 2019. <a target="_blank" rel="noopener" href="http://arxiv.org/abs/1810.03234">http://arxiv.org/abs/1810.03234</a>.</li>
<li>Chazal, Fr√©d√©ric, and Bertrand Michel. ‚ÄúAn Introduction to Topological Data Analysis: Fundamental and Practical Aspects for Data Scientists.‚Äù Frontiers in Artificial Intelligence 4 (September 29, 2021): 667963. <a target="_blank" rel="noopener" href="https://doi.org/10.3389/frai.2021.667963">https://doi.org/10.3389/frai.2021.667963</a>.</li>
<li>Murugan, Jeff, and Duncan Robertson. ‚ÄúAn Introduction to Topological Data Analysis for Physicists: From LGM to FRBs.‚Äù arXiv, April 24, 2019. <a target="_blank" rel="noopener" href="http://arxiv.org/abs/1904.11044">http://arxiv.org/abs/1904.11044</a>.</li>
</ul>
<p>Supports for neural&#x2F;cognitive science side:</p>
<ul>
<li>Dabaghian, Y., F. M√©moli, L. Frank, and G. Carlsson. ‚ÄúA Topological Paradigm for Hippocampal Spatial Map Formation Using Persistent Homology.‚Äù Edited by Ila Fiete. PLoS Computational Biology 8, no. 8 (August 9, 2012): e1002581. <a target="_blank" rel="noopener" href="https://doi.org/10.1371/journal.pcbi.1002581">https://doi.org/10.1371/journal.pcbi.1002581</a>.</li>
<li>Remington, Evan D., Devika Narain, Eghbal A. Hosseini, and Mehrdad Jazayeri. ‚ÄúFlexible Sensorimotor Computations through Rapid Reconfiguration of Cortical Dynamics.‚Äù Neuron 98, no. 5 (June 2018): 1005-1019.e5. <a target="_blank" rel="noopener" href="https://doi.org/10.1016/j.neuron.2018.05.020">https://doi.org/10.1016/j.neuron.2018.05.020</a>.</li>
<li>Yu, Byron M., John P. Cunningham, Gopal Santhanam, Stephen I. Ryu, Krishna V. Shenoy, and Maneesh Sahani. ‚ÄúGaussian-Process Factor Analysis for Low-Dimensional Single-Trial Analysis of Neural Population Activity.‚Äù Journal of Neurophysiology 102, no. 1 (July 2009): 614‚Äì35. <a target="_blank" rel="noopener" href="https://doi.org/10.1152/jn.90941.2008">https://doi.org/10.1152/jn.90941.2008</a>.</li>
<li>Shine, James M., Michael Breakspear, Peter T. Bell, Kaylena A. Ehgoetz Martens, Richard Shine, Oluwasanmi Koyejo, Olaf Sporns, and Russell A. Poldrack. ‚ÄúHuman Cognition Involves the Dynamic Integration of Neural Activity and Neuromodulatory Systems.‚Äù Nature Neuroscience 22, no. 2 (February 2019): 289‚Äì96. <a target="_blank" rel="noopener" href="https://doi.org/10.1038/s41593-018-0312-0">https://doi.org/10.1038/s41593-018-0312-0</a>.</li>
<li>Peyrache, Adrien, Marie M Lacroix, Peter C Petersen, and Gy√∂rgy Buzs√°ki. ‚ÄúInternally Organized Mechanisms of the Head Direction Sense.‚Äù Nature Neuroscience 18, no. 4 (April 2015): 569‚Äì75. <a target="_blank" rel="noopener" href="https://doi.org/10.1038/nn.3968">https://doi.org/10.1038/nn.3968</a>.</li>
<li>Gallego, Juan A., Matthew G. Perich, Raeed H. Chowdhury, Sara A. Solla, and Lee E. Miller. ‚ÄúLong-Term Stability of Cortical Population Dynamics Underlying Consistent Behavior.‚Äù Nature Neuroscience 23, no. 2 (February 2020): 260‚Äì70. <a target="_blank" rel="noopener" href="https://doi.org/10.1038/s41593-019-0555-4">https://doi.org/10.1038/s41593-019-0555-4</a>.</li>
<li>Low, Ryan J., Sam Lewallen, Dmitriy Aronov, Rhino Nevers, and David W. Tank. ‚ÄúProbing Variability in a Cognitive Map Using Manifold Inference from Neural Dynamics.‚Äù Preprint. Neuroscience, September 16, 2018. <a target="_blank" rel="noopener" href="https://doi.org/10.1101/418939">https://doi.org/10.1101/418939</a>.</li>
<li>Elsayed, Gamaleldin F., Antonio H. Lara, Matthew T. Kaufman, Mark M. Churchland, and John P. Cunningham. ‚ÄúReorganization between Preparatory and Movement Population Responses in Motor Cortex.‚Äù Nature Communications 7, no. 1 (October 27, 2016): 13239. <a target="_blank" rel="noopener" href="https://doi.org/10.1038/ncomms13239">https://doi.org/10.1038/ncomms13239</a>.</li>
<li>Bernardi, Silvia, Marcus K. Benna, Mattia Rigotti, J√©r√¥me Munuera, Stefano Fusi, and C. Daniel Salzman. ‚ÄúThe Geometry of Abstraction in the Hippocampus and Prefrontal Cortex.‚Äù Cell 183, no. 4 (November 2020): 954-967.e21. <a target="_blank" rel="noopener" href="https://doi.org/10.1016/j.cell.2020.09.031">https://doi.org/10.1016/j.cell.2020.09.031</a>.</li>
<li>Chaudhuri, Rishidev, Berk Ger√ßek, Biraj Pandey, Adrien Peyrache, and Ila Fiete. ‚ÄúThe Intrinsic Attractor Manifold and Population Dynamics of a Canonical Cognitive Circuit across Waking and Sleep.‚Äù Nature Neuroscience 22, no. 9 (September 2019): 1512‚Äì20. <a target="_blank" rel="noopener" href="https://doi.org/10.1038/s41593-019-0460-x">https://doi.org/10.1038/s41593-019-0460-x</a>.</li>
<li>Singh, G., F. Memoli, T. Ishkhanov, G. Sapiro, G. Carlsson, and D. L. Ringach. ‚ÄúTopological Analysis of Population Activity in Visual Cortex.‚Äù Journal of Vision 8, no. 8 (June 1, 2008): 11‚Äì11. <a target="_blank" rel="noopener" href="https://doi.org/10.1167/8.8.11">https://doi.org/10.1167/8.8.11</a>.</li>
</ul>
<h3 id="Diffusion-model-and-Alpha-Go-information-flow-and-search-tree"><a href="#Diffusion-model-and-Alpha-Go-information-flow-and-search-tree" class="headerlink" title="Diffusion model and Alpha Go, information flow and search tree"></a>Diffusion model and Alpha Go, information flow and search tree</h3><p>The <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Diffusion_model">diffusion model</a> is wonderful invention in the field of image generation. Combining the generator and discriminator into one. And by using noise, dividing and conquer hard question, in the same time, making a path to desired image.</p>
<p><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/AlphaGo">Alpha Go</a> is an amazing example of reinforcement learning. Use the logic&#x2F;rule of go, build a search tree, finding most valuable attempts in uncountable possibility of go game.</p>
<p>Both model involve the idea of divide complex tasks into simple ones and build a step by step path to success. Diffusion model shows us how noise can elegantly reverse the process of building information. Alpha go shows us how to build search tree for valuable data enhancement.</p>
<p>The enlightenment for us is that, instead of placing the pawns, we place noise to the problem&#x2F;task&#x2F;game we are studying about, those the best path until it reaches a completely zero information (total random). Then use machine learning to study the reverse step!</p>
<p>The applications&#x2F;experiments we should do are:</p>
<ul>
<li>Apply the idea to LLM for the issue of illusion. With the dataset of question answering as example, we can use a simple tool to embed it, such as Word2Vec or GloVe, then we randomly add small noise to words, for instance ‚ÄúFrench capital is Paris‚Äù -&gt; ‚ÄúFrench capital is London‚Äù -&gt; ‚ÄúFrench baguette is London‚Äù. Every time it gets more illogical. Then we reverse and train model to make illogical more logical.</li>
</ul>
<h3 id="Copy-drosophila-neural-group-to-NN"><a href="#Copy-drosophila-neural-group-to-NN" class="headerlink" title="Copy drosophila neural group to NN"></a>Copy drosophila neural group to NN</h3><p>One of the most excellent achievement from 2023 in science is that, scientists have completed the most advanced map of an insect brain to date, a landmark achievement in neuroscience that brings scientists closer to a true understanding of the mechanics of the mind.</p>
<div align="center">
<img src="https://api.hub.jhu.edu/factory/sites/default/files/styles/soft_crop_1600/public/2023-03/neuron1.jpg" width="400">
</div>

<p>Now, we know how many cells are there and how many connects are built. Based on this, I wish to build a neural network, probably not bipartite and we needs <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Boltzmann_machine">Boltzmann machine</a>, thus, the adjoint matrices will be its parameter.</p>
<p>We can bound the adjoint matrices, and therefore the graph connection, by making it a zero-one matrix according to drosophila neural cell connection. It should be reasonable to assume this structure is already a local minimum, at least. That gives a plenty of things to test.</p>
<ul>
<li>Can we try to run CNN or masked auto-encoders on this net? Which one is better fitting? Does this means, in drosophila, there might be masked mechanism?</li>
<li>If we assume the model parameters take value continuously or discrete, which is better? In another words, in drosophila, is it digital or analog signal when processing?</li>
<li>Can we run a simulated fruit flies in simulated environment?</li>
</ul>
<p>References</p>
<ul>
<li>Winding, M., Pedigo, B. D., Barnes, C. L., Patsolic, H. G., Park, Y., Kazimiers, T., ‚Ä¶ &amp; Zlatic, M. (2023). The connectome of an insect brain. Science, 379(6636), eadd9330.</li>
<li>Eichler, K., Li, F., Litwin-Kumar, A., Park, Y., Andrade, I., Schneider-Mizell, C. M., ‚Ä¶ &amp; Cardona, A. (2017). The complete connectome of a learning and memory centre in an insect brain. Nature, 548(7666), 175-182.</li>
</ul>
<h3 id="Masked-Autoencoders-to-be-continue"><a href="#Masked-Autoencoders-to-be-continue" class="headerlink" title="Masked Autoencoders (to be continue.)"></a>Masked Autoencoders (to be continue.)</h3></article></div></main><div class="nav-wrap"><div class="nav"><button class="site-nav"><div class="navicon"></div></button><ul class="nav_items"><li class="nav_item"><a class="nav-page" href="/"> About</a></li><li class="nav_item"><a class="nav-page" href="/#Publications"> Publications</a></li><li class="nav_item"><a class="nav-page" href="https://yilin-bao.github.io/blog/"> Blog and Thinking</a></li><li class="nav_item"><a class="nav-page" href="/#Proposals"> Research Proposals</a></li></ul></div><div class="cd-top"><i class="fa fa-arrow-up" aria-hidden="true"></i></div></div><footer id="page_footer"><div class="footer_wrap"><div class="copyright">&copy;2023 - 2024 by Yilin Bao</div><div class="theme-info">Powered by <a target="_blank" href="https://hexo.io" rel="nofollow noopener">Hexo</a> & <a target="_blank" href="https://github.com/PhosphorW/hexo-theme-academia" rel="nofollow noopener">Academia Theme</a></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery-pjax@latest/jquery.pjax.min.js"></script><script src="/js/main.js"></script></body></html>