<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>Yilin Bao's Site</title><meta name="author" content="Yilin Bao"><link rel="shortcut icon" href="/img/channels4_profile.jpeg"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.13.0/css/all.min.css"><meta name="generator" content="Hexo 7.0.0"></head><body><header id="page_header"><div class="header_wrap"><div id="blog_name"><a class="blog_title" id="site-name" href="/">Yilin Bao's Site</a></div><button class="menus_icon"><div class="navicon"></div></button><ul class="menus_items"><li class="menus_item"><a class="site-page" href="/" target="_blank"> About</a></li><li class="menus_item"><a class="site-page" href="/#Publications" target="_blank"> Publications</a></li><li class="menus_item"><a class="site-page" href="https://yilin-bao.github.io/blog/" target="_blank"> Blog and Thinking</a></li></ul></div></header><main id="page_main"><div class="side-card sticky"><div class="card-wrap" itemscope itemtype="http://schema.org/Person"><div class="author-avatar"><img class="avatar-img" src="/img/channels4_profile.jpeg" onerror="this.onerror=null;this.src='/img/channels4_profile.jpeg'" alt="avatar"></div><div class="author-discrip"><h3>Yilin Bao</h3><p class="author-bio">Your biography can be writed down here.</p></div><div class="author-links"><button class="btn m-social-links">Links</button><ul class="social-icons"><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-twitter" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-facebook-square" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-github" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-stack-overflow" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-linkedin" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-weibo" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-weixin" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-qq" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fas fa-envelope" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fas fa-rss" aria-hidden="true"></i></a></li></ul><ul class="social-links"><li><a class="e-social-link" href="/" target="_blank"><i class="fas fa-graduation-cap" aria-hidden="true"></i><span>Google Scholar</span></a></li><li><a class="e-social-link" href="/" target="_blank"><i class="fab fa-orcid" aria-hidden="true"></i><span>ORCID</span></a></li></ul></div><a class="cv-links" href="/attaches/Yilin%20Bao%20CV%20Dec%2015.docx.pdf" target="_blank"><i class="fas fa-file-pdf" aria-hidden="true"><span>My Detail CV.</span></i></a></div></div><div class="page" itemscope itemtype="http://schema.org/CreativeWork"><h2 class="page-title">Hello World</h2><article><h2 id="Manifold-Topology-Machine-learning"><a href="#Manifold-Topology-Machine-learning" class="headerlink" title="Manifold &amp; Topology &amp; Machine learning"></a>Manifold &amp; Topology &amp; Machine learning</h2><p>It is no doubt that when we say we are dealing with machine learning with neural networks (and any other non-linear autoencoders), we are dealing with manifolds in high dimensional and sparse data space. Unlike traditional natural science problems, for which they usually don’t have a very high dimensional phase space (for instance, 2n dimensional for Lagrangian), problems that appear actively in neural network articles, such as image processing and natural language processing, all have very complicated logical relationships, so comprehensive embedding representations are required. This makes it so hard to explain how the neural networks really solve the problem.</p>
<p>Traditionally, we list differential equations about the problem we want to investigate, use mathematical tools to find its eigenvectors, differential equations describe the problem, and eigenvectors are independent factors we can investigate about the problem. Now, every layer is embedded in a huge space that is really hard for our brain even trying to fantasize about it.</p>
<p>A good example for this is the Ising model (1925), for which we put the atoms in a grid and see how their magnetic dipole moment changes through time. This model, but since the magnetic dipole moment is not such a high dimensional variable (in the original model, it takes value from +1 and -1). Many years later, when RNN is proposed,</p>
<p>People invented so many ways to help understanding this huge black box. Well, usually, the input and output, for easier to use, are readable format (language, words, images, multimedia). But how about intermediate steps?</p>
<p>For instance, the more commonly used perturbation method attribution is the occlusion method, zero out (shade) part of the input pixels and observe the changes in the output. The greater the change in output, the stronger the importance of this part (Ancona, Marco, et al.).</p>
<p>Interestly, there is something called topology. Topology is a branch of mathematics that explores the invariance properties of spatial shapes and structures under continuous deformations. It differs from geometry, which focuses on specific measurements of shapes, by concentrating. Abstract, not only means it is hard to learn, but also means it cuts out a lot of insignificant detail just like the Ockham’s Razor, leading us to what is essential.</p>
<p><img src="https://cdn.mathpix.com/snip/images/StQYfimnFpoGKCrqEjmpR4MCt1uOmHZmkfPB2lJnZww.original.fullsize.png"></p>
<p>There are already a lot of wonderful works on introducing topology to neural networks. Some people try to analyze the weight matrices similar to how we study adjoint matrices in graph theory (Zhao, Yang, and Hao Zhang). And some people took out many different rounds of training, and observed in total how those weights and kernels (for CNN) converge and are there any interesting facts to be notified (Gabrielsson, Rickard Brüel, and Gunnar Carlsson).</p>
<p><img src="https://cdn.mathpix.com/snip/images/HW5TKEs_hven7Qpj_blZ0rnoPeueLGVhKnG-tkKz3EI.original.fullsize.png"></p>
<p>Think about this, we input something in vector representation, and hope neural networks can help us to learn how this can be properly mapped to another thing, output, also in vector representation. This is from one high dimensional manifold to another high dimensional manifold. And the rule is? Defined by dataset and loss function, we point out every part of the manifold should be located a where in the new manifold.</p>
<p><img src="https://cdn.mathpix.com/snip/images/uS1RD9_Ajkjf6Hc0yzuoP8-c4juEi1mjSFly9iaSkXY.original.fullsize.png"></p>
<p><img src="https://cdn.mathpix.com/snip/images/SINDyRizd0VfBy2n1bdiPTTLFLFwdFuriygIsJ90XjY.original.fullsize.png"></p>
<ul>
<li>Ancona, Marco, et al. “Towards better understanding of gradient-based attribution methods for deep neural networks.” arXiv preprint arXiv:1711.06104 (2017).</li>
<li>Zhao, Yang, and Hao Zhang. “A Topological Approach to Exploring Convolutional Neural Networks.” arXiv, November 2, 2020. <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2011.00789">http://arxiv.org/abs/2011.00789.</a></li>
<li>Gabrielsson, Rickard Brüel, and Gunnar Carlsson. “Exposition and Interpretation of the Topology of Neural Networks.” arXiv, October 18, 2019. <a target="_blank" rel="noopener" href="http://arxiv.org/abs/1810.03234">http://arxiv.org/abs/1810.03234.</a></li>
<li>Zhao, Yang, and Hao Zhang. “A Topological Approach to Exploring Convolutional Neural Networks.” arXiv, November 2, 2020. <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2011.00789">http://arxiv.org/abs/2011.00789.</a></li>
</ul>
</article></div></main><div class="nav-wrap"><div class="nav"><button class="site-nav"><div class="navicon"></div></button><ul class="nav_items"><li class="nav_item"><a class="nav-page" href="/"> About</a></li><li class="nav_item"><a class="nav-page" href="/#Publications"> Publications</a></li><li class="nav_item"><a class="nav-page" href="https://yilin-bao.github.io/blog/"> Blog and Thinking</a></li></ul></div><div class="cd-top"><i class="fa fa-arrow-up" aria-hidden="true"></i></div></div><footer id="page_footer"><div class="footer_wrap"><div class="copyright">&copy;2023 by Yilin Bao</div><div class="theme-info">Powered by <a target="_blank" href="https://hexo.io" rel="nofollow noopener">Hexo</a> & <a target="_blank" href="https://github.com/PhosphorW/hexo-theme-academia" rel="nofollow noopener">Academia Theme</a></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery-pjax@latest/jquery.pjax.min.js"></script><script src="/js/main.js"></script></body></html>